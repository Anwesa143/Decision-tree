{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- A decision tree is a supervised learning algorithm used for both classification and regression. In classification, it works by recursively partitioning the data into subsets based on feature values, creating a tree-like structure that predicts the class of a new data point based on its features.\n",
        "How Decision Trees Work?\n",
        "1. Start with the Root Node: It begins with a main question at the root node which is derived from the dataset’s features.\n",
        "\n",
        "2. Ask Yes/No Questions: From the root, the tree asks a series of yes/no questions to split the data into subsets based on specific attributes.\n",
        "\n",
        "3. Branching Based on Answers: Each question leads to different branches:\n",
        "\n",
        "If the answer is yes, the tree follows one path.\n",
        "If the answer is no, the tree follows another path.\n",
        "4. Continue Splitting: This branching continues through further decisions helps in reducing the data down step-by-step.\n",
        "\n",
        "5. Reach the Leaf Node: The process ends when there are no more useful questions to ask leading to the leaf node where the final decision or prediction is made.\n",
        "\n",
        "2.: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "- Gini Impurity and Entropy are both measures of impurity used in decision tree algorithms to determine the best way to split data at each node. They quantify how \"mixed\" or \"pure\" a node is in terms of the classes present. The goal of a decision tree is to minimize impurity at each split, creating nodes that are increasingly pure as you move down the tree.\n",
        "\n",
        "Gini Impurity:\n",
        "Measures the probability of misclassifying a randomly chosen element in a dataset if it were randomly labeled according to the class distribution within that node.\n",
        "Ranges from 0 to 0.5 in binary classification. A Gini impurity of 0 indicates perfect purity (all data points belong to the same class), while 0.5 represents maximum impurity (equal distribution of classes).\n",
        " Faster to compute than entropy because it doesn't involve logarithmic calculations.\n",
        "\n",
        "Entropy:\n",
        "Measures the amount of uncertainty or disorder in a dataset.\n",
        "Also ranges from 0 to 1. 0 indicates a pure node, and 1 indicates maximum impurity.\n",
        "Provides a more nuanced measure of impurity compared to Gini impurity but is computationally more expensive.\n",
        "\n",
        "Impact on Splits in a Decision Tree:\n",
        "- Finding the best split:\n",
        "Decision trees use these impurity measures to evaluate different possible splits at each node.\n",
        "- Minimizing impurity:\n",
        "The algorithm selects the split that results in the lowest Gini impurity or entropy for the resulting child nodes.\n",
        "- Information Gain:\n",
        "When using entropy, the concept of information gain is crucial. Information gain is the reduction in entropy achieved by splitting on a particular feature. The feature with the highest information gain is preferred.\n",
        "- Greedy Approach:\n",
        "Decision trees use a greedy approach, meaning they make the best split at each node locally, without considering the global structure of the tree.\n",
        "- Iterative Process:\n",
        "The process of calculating impurity and selecting splits is repeated recursively for each child node until a stopping criterion is met (e.g., maximum depth reached, minimum number of samples in a node).\n",
        "\n",
        "3.What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "-  \n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "Definition: In pre-pruning, the decision tree stops growing early during its construction if a certain condition is met (like maximum depth reached, minimum number of samples at a node, or minimum information gain).\n",
        "\n",
        "Goal: Prevents the tree from becoming too large and overfitting during training.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Efficiency: Pre-pruning reduces training time and memory usage because the tree does not grow unnecessarily deep. This is especially useful when working with large datasets.\n",
        "\n",
        "Post-Pruning (Pruning after Full Growth)\n",
        "\n",
        "Definition: In post-pruning, the decision tree is first grown fully (allowing it to overfit), and then it is simplified by removing branches or subtrees that do not provide significant predictive power (using validation data or statistical tests).\n",
        "\n",
        "Goal: Improves generalization by reducing overfitting after seeing the full structure.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Better Accuracy: Post-pruning usually results in a more accurate model on unseen data, since it considers the entire tree and prunes only those parts that are harmful to generalization\n",
        "\n",
        "4.What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "- Information Gain (IG) in Decision Trees:\n",
        "Information Gain is a metric used in decision trees to decide which feature to split on at each step. It measures the reduction in uncertainty (or impurity) about the target variable when a dataset is split on a given attribute.\n",
        "Imortance:\n",
        "Guides Feature Selection:\n",
        "It helps the decision tree algorithm pick the attribute that gives the purest child nodes (less mixed, more class-homogeneous).\n",
        "Prevents Random Splits:\n",
        "Without IG, splits could be chosen arbitrarily. IG ensures the tree grows in a way that maximizes learning at each step.\n",
        "Leads to Better Accuracy:\n",
        "By always picking the attribute that provides the most information, the decision tree reduces classification error.\n",
        "Balances Tree Growth:\n",
        "It avoids unnecessary complexity by favoring splits that contribute meaningful separation of classes.\n",
        "\n",
        "5.What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        " Decision trees are widely used in both classification and regression tasks because they are interpretable and easy to implement.\n",
        "Healthcare & Medicine:\n",
        "-  diseases (e.g., predicting whether a patient has diabetes based on test results).\n",
        "- Treatment recommendation systems.\n",
        "- Risk prediction (e.g., likelihood of heart disease).\n",
        "Finance & Banking:\n",
        "- Credit risk assessment (approve/deny loan).\n",
        "- Fraud detection in transactions.\n",
        "- Customer segmentation for targeted offers.\n",
        "Retail & Marketing:\n",
        "- Predicting customer churn (who is likely to leave a service).\n",
        "- Product recommendation.\n",
        "- Sales forecasting.\n",
        "Human Resources:\n",
        "Employee attrition prediction.\n",
        "Hiring decision support.\n",
        "Manufacturing & Operations:\n",
        "Predictive maintenance (when a machine might fail).\n",
        "Quality control (defect classification).\n",
        "\n",
        "Advantages of Decision Trees\n",
        "\n",
        "- Easy to Understand & Interpret:\n",
        "Looks like a flowchart → even non-technical people can follow the reasoning.\n",
        "\n",
        "- Handles Both Numerical & Categorical Data:\n",
        "Works on a mix of feature types without much preprocessing.\n",
        "\n",
        "- No Need for Feature Scaling:\n",
        "Unlike SVM or Logistic Regression, decision trees don’t require normalization/standardization.\n",
        "Limitations of Decision Trees\n",
        "\n",
        "- Overfitting (High Variance):\n",
        "Trees can become too complex, memorizing training data instead of generalizing.\n",
        "\n",
        "- Solution: pruning, max depth, or ensemble methods like Random Forest.\n",
        "\n",
        "- Unstable to Small Changes in Data:\n",
        "A small change in training data can lead to a very different tree structure.\n",
        "\n",
        "- Biased Towards Features with Many Levels:\n",
        "Features with more unique values (e.g., ID numbers) can dominate splits.\n",
        "\n"
      ],
      "metadata": {
        "id": "PNOKFLakS9__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "oWH3BWjSgNly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "tree_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_depth3.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy for max_depth=3 tree\n",
        "y_pred_depth3 = tree_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Train a fully-grown Decision Tree (no max_depth specified)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy for fully-grown tree\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_depth3)\n",
        "print(\"Accuracy with fully-grown tree:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zAKid2IgTyj",
        "outputId": "401336c3-480b-40a2-886c-566bdd6f7f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 0.9666666666666667\n",
            "Accuracy with fully-grown tree: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "UwgX4sFHgjmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4VHuN3aglb_",
        "outputId": "d82bdfdb-0719-4840-87b8-4640ddb4c987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "EpDeMYVueu-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],   # Try different tree depths\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]  # Min samples required to split\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy on Test Data:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNLZ-TbLfURc",
        "outputId": "b467abaa-8d9c-47ab-db4d-2d7f12013871"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Model Accuracy on Test Data: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "- 1. Handle the Missing Values\n",
        "\n",
        "Identify missing data: Use methods like df.isnull().sum() to see which features have missing values and their percentage.\n",
        "Strategies:\n",
        "- Numerical features: Impute with mean/median (if data is symmetric/skewed), or use KNN imputation for more accuracy.\n",
        "- Categorical features: Impute with mode (most frequent value), or use a special category like \"Unknown\".\n",
        "- High missing rate (>40%): Consider dropping such columns if they don’t add much value.\n",
        "2. Encode the Categorical Features\n",
        "Decision Trees handle categorical data in split-based form, but sklearn’s implementation requires numerical encoding.\n",
        "Options:\n",
        "- One-Hot Encoding: For nominal categories (e.g., blood type: A, B, AB, O).\n",
        "- Ordinal Encoding: For ordered categories (e.g., disease stage: mild < moderate < severe).\n",
        "Use ColumnTransformer in sklearn to apply different encodings to different feature types in one pipeline.\n",
        "3. Train a Decision Tree Model\n",
        "- Split the dataset into train/test sets (e.g., 70/30 split) or use stratified k-fold cross-validation (important if classes are imbalanced).\n",
        "4. Tune Hyperparameters\n",
        "Decision Trees can overfit if not pruned. Use GridSearchCV or RandomizedSearchCV.\n",
        "Key hyperparameters:\n",
        "max_depth: Controls tree depth.\n",
        "min_samples_split: Minimum samples to split a node.\n",
        "min_samples_leaf: Minimum samples at a leaf node.\n",
        "criterion: \"gini\" or \"entropy\" (impurity measure).\n",
        "5. Evaluate Model Performance\n",
        "Use multiple metrics (accuracy alone can be misleading in healthcare):\n",
        "Confusion Matrix (TP, FP, TN, FN).\n",
        "Precision (how many predicted positives are correct).\n",
        "Recall / Sensitivity (how many actual positives are detected).\n",
        "F1-score (balance between precision and recall).\n",
        "ROC-AUC (probability model ranks a random positive higher than a random negative).\n",
        "6. Business Value in Healthcare\n",
        "- Early Diagnosis: Helps flag high-risk patients for further medical tests.\n",
        "Resource Allocation: Hospitals can prioritize limited resources (ICU beds, diagnostic tests) for high-risk patients.\n",
        "- Personalized Treatment: Assists doctors in tailoring treatments based on predicted risk.\n",
        "- Cost Savings: Reduces unnecessary tests for low-risk patients, saving both time and money.\n",
        "- Patient Outcomes: Ultimately improves survival rates by catching diseases earlier.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ueJnwUafrfa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfoS7cwyfyNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ayf2M5LS3W2"
      },
      "outputs": [],
      "source": []
    }
  ]
}